{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "360644ce-5466-4f37-bfc2-29c7af859dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 定义一个函数来清洗非法字符\n",
    "def clean_text(text):\n",
    "    ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "    return ILLEGAL_CHARACTERS_RE.sub('', str(text))\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r\"C:\\Users\\XiLeo\\Desktop\\Soc Media Acqu\\5507 网易云原创榜 GROUP14\\5507 数据文件\\原创榜总comments_data.csv\")\n",
    "\n",
    "# 使用 DataFrame.map 方法清洗 DataFrame 中的数据\n",
    "df = df.map(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "# 去除重复的ID\n",
    "df = df.drop_duplicates(subset='user_id')\n",
    "\n",
    "# 将age列转换为数值型，无法转换的设置为NaN\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "# 去除无年龄信息的记录\n",
    "df = df.dropna(subset=['age'])\n",
    "\n",
    "# 筛选年龄在10到55岁之间的记录\n",
    "df = df[(df['age'] >= 10) & (df['age'] <= 55)]\n",
    "\n",
    "# 保存清理后的数据到新的Excel文件\n",
    "df.to_excel('清理后的数据年龄.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e434cb32-e116-40d2-95ac-c0cc3a24707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r\"C:\\Users\\XiLeo\\Desktop\\Soc Media Acqu\\5507 网易云原创榜 GROUP14\\5507 数据文件\\原创榜总comments_data.csv\")\n",
    "\n",
    "# 去除重复的ID\n",
    "df = df.drop_duplicates(subset='user_id')\n",
    "\n",
    "# 筛选gender列中值为1和2的记录\n",
    "df = df[df['gender'].isin([1, 2])]\n",
    "\n",
    "# 保存清理后的数据到新的Excel文件\n",
    "df.to_excel('清理后的数据性别.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d21dd5-a4c0-4008-9721-7cb153e78b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始DataFrame的大小: (15207, 10)\n",
      "去重后DataFrame的大小: (9587, 10)\n",
      "筛选后DataFrame的大小: (0, 10)\n",
      "没有数据满足条件，未能生成Excel文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r\"C:\\Users\\XiLeo\\Desktop\\Soc Media Acqu\\5507 网易云原创榜 GROUP14\\5507 数据文件\\原创榜总comments_data.csv\")\n",
    "\n",
    "# 打印原始DataFrame的大小\n",
    "print(\"原始DataFrame的大小:\", df.shape)\n",
    "\n",
    "# 去除重复的ID\n",
    "df = df.drop_duplicates(subset='user_id')\n",
    "\n",
    "# 打印去重后DataFrame的大小\n",
    "print(\"去重后DataFrame的大小:\", df.shape)\n",
    "\n",
    "# 筛选gender列中值为1和2的记录\n",
    "df = df[df['gender'].isin([1, 2])]\n",
    "\n",
    "# 打印筛选后DataFrame的大小\n",
    "print(\"筛选后DataFrame的大小:\", df.shape)\n",
    "\n",
    "# 如果DataFrame不为空，则保存清理后的数据到新的Excel文件\n",
    "if not df.empty:\n",
    "    df.to_excel('清理后的数据.xlsx', index=False)\n",
    "else:\n",
    "    print(\"没有数据满足条件，未能生成Excel文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21f96e96-504f-44b3-afb9-b1ad0627d3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始DataFrame的大小: (15207, 10)\n",
      "去重后DataFrame的大小: (9587, 10)\n",
      "gender列中的唯一值: ['0' '1' '2' nan '2024/10/30 21:18' '2024/10/30 21:17' '2024/10/30 21:07'\n",
      " '2024/10/23 19:20' '2024/10/22 9:28' '2024/10/22 9:27' '2024/10/21 21:18'\n",
      " '2024/10/19 0:50' '2024/10/17 22:41' '2024/10/13 16:30' '2024/10/4 15:42'\n",
      " '2024/10/4 15:41' '2024/10/4 15:35' '2024/10/2 10:34' '2024/9/22 11:23'\n",
      " '2024/9/22 11:10' '2024/9/21 11:24' '2024/9/19 6:42' '2024/9/19 6:41'\n",
      " '2024/8/12 20:46' '2024/8/10 7:53' '2024/8/8 14:32' '2024/8/8 8:22'\n",
      " '2024/8/7 16:57' '2024/8/6 20:55' '2024/8/5 19:19' '2024/8/5 19:18'\n",
      " '2024/8/5 19:15' '2024/8/5 19:10' '2024/8/5 19:05' '2024/7/31 20:17'\n",
      " '2024/7/31 9:41' '2024/7/30 13:42' '2024/7/27 19:40' '2024/7/27 19:39'\n",
      " '2024/7/9 23:08' '2024/7/9 23:07' '2024/7/9 23:06' '2024/7/7 8:05'\n",
      " '2024/7/2 14:33' '2024/7/2 13:31' '2024/6/22 7:35' '2024/6/19 18:30'\n",
      " '2024/6/15 17:38' '2024/6/11 0:36' '2024/6/7 19:44' '无' '2024/6/4 10:23'\n",
      " '2024/6/1 17:59' '2024/5/31 18:53' '2024/5/27 21:24' '2024/5/27 21:21'\n",
      " '2024/5/25 11:44' '2024/5/24 21:34' '2024/5/18 15:20' '2024/5/15 12:29'\n",
      " '2024/5/3 23:09' '2024/4/29 23:47' '2024/4/9 17:55' '2024/4/7 19:33'\n",
      " '2024/3/31 21:29' '2024/3/31 21:28' '2024/3/31 21:27' '2024/3/29 14:17'\n",
      " '2024/3/28 23:07' '2024/3/27 20:38' '2024/3/22 0:31' '2024/3/17 21:46'\n",
      " '2024/3/14 18:57' '2024/3/10 22:29' '2024/2/20 16:06' '2024/2/19 22:51'\n",
      " '2024/2/19 18:48' '2024/2/19 14:41' '2024/2/18 12:50' '2024/2/15 17:50'\n",
      " '2024/2/9 10:03' '2024/2/4 21:45' '2024/1/27 10:31' '2024/1/26 20:38'\n",
      " '2024/1/20 12:04' '2024/1/14 11:47' '2024/1/13 13:56' '2024/1/11 22:54'\n",
      " '2024/1/6 17:02' '2024/1/6 10:04' '2024/1/5 10:00' '2023/12/29 19:05'\n",
      " '2023/12/26 13:12' '2023/12/23 21:38' '2023/12/15 8:54'\n",
      " '2023/12/14 19:09' '2023/12/14 16:11' '2023/12/9 7:37' '2023/12/2 16:56'\n",
      " '2023/11/19 13:44' '2023/11/18 15:43' '2023/11/11 13:34'\n",
      " '2023/11/4 17:57' '2023/10/27 20:26' '2023/10/5 18:15' '2023/10/1 7:36'\n",
      " '2023/9/29 10:52' '2023/9/23 7:57' '2023/9/22 23:10' '2023/9/21 20:22'\n",
      " '2023/9/18 20:03' '2023/9/17 10:28' '2023/9/11 10:05' '2023/9/5 7:49'\n",
      " '2023/8/27 9:35' '2023/8/26 10:15' '2023/7/10 17:58' '2023/6/30 22:31'\n",
      " '2023/6/10 20:53' '2023/6/10 20:52' '2023/5/15 20:45' '2023/5/13 21:06'\n",
      " '2023/4/23 10:24' '2023/4/23 10:21' '2023/4/23 10:20' '2023/4/23 10:19'\n",
      " '2023/4/23 10:18' '2023/1/12 22:37' '2022/12/13 7:12' '2022/12/4 14:24'\n",
      " '2022/8/19 11:45' '2022/7/30 10:40' '2022/7/25 17:48' '2022/7/6 22:24'\n",
      " '2022/6/30 22:13' '2022/5/13 15:16' '2022/5/6 10:38' '2022/5/6 10:34'\n",
      " '2022/3/18 23:31' '2022/3/15 16:27' '2022/3/5 20:05' '2022/2/22 15:54'\n",
      " '2022/2/18 9:09' '2022/2/12 13:18' '2022/2/6 17:37' '2022/1/27 15:19'\n",
      " '2022/1/27 12:44' '2022/1/27 10:28' '2022/1/27 9:51' '2022/1/17 23:33'\n",
      " '2021/12/18 14:15' '2021/12/14 7:47' '2021/11/7 1:46' '2021/9/18 22:54'\n",
      " '2021/9/10 12:36' '2021/8/20 18:03' '2021/8/17 21:37' '2021/6/8 9:08'\n",
      " '2021/5/23 11:00' '2021/4/3 0:15' '2021/3/23 22:37' '2021/3/7 21:39'\n",
      " '2021/3/4 23:45' '2021/2/18 20:37' '2021/2/17 21:02' '2021/2/6 12:52'\n",
      " '2021/1/5 16:59' '2020/12/21 16:52' '2020/11/27 3:56' '2020/9/19 16:59'\n",
      " '2020/9/2 8:23' '2020/8/28 18:28' '2020/8/28 13:31' '2020/8/15 11:39'\n",
      " '2020/7/17 13:23' '2020/7/17 8:49' '2020/7/17 7:13' '2020/7/12 15:50'\n",
      " '2020/7/4 13:57' '2020/6/20 6:12' '2020/6/2 17:16' '2020/5/30 13:45'\n",
      " '2020/5/28 21:04' '2020/5/24 15:18' '2020/5/16 18:51' '2020/5/11 12:07'\n",
      " '2020/5/9 5:15' '2020/5/8 22:03' '2020/5/7 15:26' '2020/5/7 11:14'\n",
      " '2020/5/1 11:27' '2020/4/28 20:27' '2020/4/21 1:02' '2020/4/20 11:31'\n",
      " '2020/4/19 12:02' '2020/4/16 11:48' '2020/4/12 0:31' '2020/4/11 18:18'\n",
      " '2020/3/24 20:54' '2020/3/20 17:26' '2020/3/18 0:55' '2020/3/16 20:35'\n",
      " '2020/2/27 13:47' '2020/2/21 23:29' '2020/2/19 16:20' '2020/2/14 2:10'\n",
      " '2020/2/12 14:43' '2020/2/10 0:03' '2020/2/8 10:13' '2020/2/5 10:33'\n",
      " '2020/2/4 0:49' '2020/2/1 23:22' '2020/1/29 21:01' '2020/1/29 1:03'\n",
      " '2020/1/25 19:49' '2020/1/15 18:30' '2020/1/11 14:59' '2020/1/10 23:22'\n",
      " '2020/1/3 16:39' '2020/1/1 15:10' '2020/1/1 13:13' '2019/12/27 8:32'\n",
      " '2019/12/20 15:21' '2019/12/15 8:33' '2019/12/5 23:17' '2019/12/5 12:04'\n",
      " '2019/12/4 22:50' '2019/11/15 20:39' '2019/10/29 23:19'\n",
      " '2019/10/25 17:01' '2019/10/11 13:07' '2019/10/2 17:44' '2019/10/1 11:09'\n",
      " '2019/10/1 0:43' '2019/9/26 16:27' '2019/9/18 19:31' '2019/9/14 9:43'\n",
      " '2019/9/14 9:42' '2019/9/14 9:32' '2019/9/13 14:55' '2019/9/1 17:44'\n",
      " '2019/9/1 0:40' '2019/8/22 15:38' '2019/8/15 17:05' '2019/8/14 18:55'\n",
      " '2019/8/7 11:05' '2019/8/7 1:39' '2019/8/6 20:04' '2019/8/6 10:02'\n",
      " '2019/7/23 19:17' '2019/7/18 21:15' '2019/7/18 3:59' '2019/7/12 22:33'\n",
      " '2019/7/5 10:20' '2019/7/1 17:24' '2019/6/29 0:36' '2019/6/27 12:17'\n",
      " '2019/6/26 23:20' '2019/6/26 1:15' '2019/6/6 19:09' '2019/6/3 14:25'\n",
      " '2019/5/31 8:35' '2019/5/30 14:23' '2019/5/25 22:16' '2019/5/16 10:31'\n",
      " '2019/5/8 14:46' '2019/4/26 14:27' '2019/4/23 21:05' '2019/4/20 9:55'\n",
      " '2019/4/19 9:20' '2019/4/17 19:34' '2019/4/17 19:31' '2019/4/15 1:29'\n",
      " '2019/4/11 7:58' '2019/4/1 21:30' '2019/3/22 15:44' '2019/3/15 1:07'\n",
      " '2019/3/9 14:18' '2019/3/8 18:03' '2019/3/5 14:56' '2019/3/2 10:24'\n",
      " '2019/2/24 18:56' '2019/2/24 13:11' '2019/2/22 13:41' '2019/2/4 20:09'\n",
      " '2019/1/29 2:58' '2018/12/31 13:50' '2018/12/17 20:55' '2018/12/13 14:27'\n",
      " '2018/12/11 14:48' '2018/11/22 22:33' '2018/11/22 21:17'\n",
      " '2018/11/22 21:13' '2018/11/22 20:46' '2018/11/22 14:07'\n",
      " '2018/11/19 11:21' '2018/11/19 11:02' '2018/11/18 21:37'\n",
      " '2018/11/18 14:01' '2018/11/16 13:00' '2018/11/14 13:11'\n",
      " '2018/11/13 12:51' '2018/11/10 10:01' '2018/11/9 23:51' '2018/11/9 8:34'\n",
      " '2018/11/8 13:28' '2018/11/8 12:54' '2018/11/8 12:48' '2018/11/8 12:47'\n",
      " '2018/11/8 12:46' '2018/11/8 12:35' '2018/11/8 12:34' '2018/11/8 12:22'\n",
      " '2018/11/8 12:00' '2018/11/8 11:59' '2018/11/8 11:57' '2018/11/5 12:02'\n",
      " '2018/11/2 18:31' '2018/11/1 8:49' '2018/10/29 12:10' '2018/10/28 13:09'\n",
      " '2018/10/28 13:07' '2018/10/27 20:55' '2018/10/20 8:51'\n",
      " '2018/10/11 22:05' '2018/10/7 14:56' '2018/9/27 13:52' '2018/9/14 18:24'\n",
      " '2018/8/28 9:38' '2018/8/26 11:49' '2018/8/26 8:14' '2018/8/24 22:59'\n",
      " '2018/8/24 17:06' '2018/8/16 20:29' '2018/8/16 20:26' '2018/8/9 19:26'\n",
      " '2018/8/2 19:56' '2018/7/30 19:08' '2018/7/30 13:47' '2018/7/30 0:41'\n",
      " '2018/7/25 18:17' '2018/7/14 10:05' '2018/7/12 13:27' '2018/7/12 12:25'\n",
      " '2018/6/29 20:38' '2018/6/19 11:22' '2018/6/12 19:26' '2018/6/12 11:04'\n",
      " '2018/6/11 11:27' '2018/6/8 22:51' '2018/6/8 9:13' '2018/6/8 9:12'\n",
      " '2018/6/7 22:28' '2018/6/7 15:31' '2018/6/6 20:33' '2018/6/6 0:00'\n",
      " '2018/6/5 6:45' '2018/6/5 6:43' '2018/6/1 8:35' '2018/5/24 17:51'\n",
      " '2018/5/24 11:15' '2018/5/19 12:45' '2018/5/17 17:44' '2018/4/27 19:59'\n",
      " '2018/4/26 20:35' '2018/4/23 15:43' '2018/4/23 13:32' '2018/4/6 0:00'\n",
      " '2018/4/5 22:49' '2018/4/4 16:06' '2018/4/4 15:33' '2018/4/3 20:41'\n",
      " '2018/3/30 18:30' '2018/3/29 18:20' '2018/3/24 23:53' '2018/3/20 17:13'\n",
      " '2018/3/14 11:17' '2018/3/6 19:18' '2018/3/5 15:16' '2018/3/1 13:58'\n",
      " '2018/2/5 16:40' '2018/1/24 15:49' '2018/1/20 10:49' '2018/1/5 0:54'\n",
      " '2017/12/8 18:35' '2017/12/1 10:10' '2017/11/30 19:34' '2017/11/30 16:12'\n",
      " '2017/11/30 10:30' '2017/11/29 16:27' '2017/11/29 14:28'\n",
      " '2017/11/13 16:24' '2017/11/6 11:21' '2017/10/11 20:49' '2017/10/5 20:36'\n",
      " '2017/9/26 13:13' '2017/9/26 13:12' '2017/9/21 20:37' '2017/8/31 20:57'\n",
      " '2017/8/26 14:44' '2017/7/20 12:06' '2017/7/12 10:16' '2017/7/11 13:59'\n",
      " '2017/7/6 23:21' '2017/7/4 23:49' '2017/7/4 23:48' '2017/6/28 22:20'\n",
      " '2017/6/23 21:57' '2017/6/19 11:34' '2017/6/5 3:10' '2017/4/14 17:16'\n",
      " '2017/4/5 17:30' '2017/3/1 18:29' '2017/2/23 9:07' '2017/1/30 16:04' '4'\n",
      " '2017/1/12 21:00' '2017/1/2 16:54' '2016/12/26 14:36' '2016/12/26 14:01'\n",
      " '2016/12/23 15:28' '2016/12/18 18:09' '2016/12/17 20:06'\n",
      " '2016/11/22 18:37' '2016/11/22 17:32' '2016/11/19 18:20'\n",
      " '2016/11/17 22:54' '2016/11/17 3:50' '2016/11/6 18:53' '2016/11/4 19:40'\n",
      " '2016/10/31 9:09' '2016/10/26 14:20' '2016/10/6 10:14' '2016/9/30 9:45'\n",
      " '2016/9/26 21:02' '2016/9/26 20:25' '2016/9/15 15:10' '2016/8/26 1:02'\n",
      " '2016/8/21 15:10' '2016/8/12 10:22' '2016/7/31 8:21' '2016/7/11 20:33'\n",
      " '2016/7/9 11:02' '2016/6/23 20:32' '2016/6/23 19:33' '2016/6/23 19:27'\n",
      " '2016/6/23 19:25' '2016/6/13 19:48' '2016/5/26 19:16' '2016/5/17 12:22'\n",
      " '2016/5/16 0:21' '2016/4/30 17:47' '2016/4/21 21:04' '2016/4/14 11:29'\n",
      " '2016/4/10 20:47' '2016/4/3 18:26' '2016/4/1 22:34' '2016/3/30 21:54'\n",
      " '2016/3/28 17:49' '2016/3/22 22:12' '2016/3/20 11:19' '2016/3/7 8:20'\n",
      " '2016/3/5 22:45' '2016/2/27 23:49' '2016/1/16 21:47' '2016/1/1 13:23'\n",
      " '2015/12/20 11:42' '2015/12/14 17:15' '2015/11/17 18:12'\n",
      " '2015/11/1 11:44' '2015/10/29 18:50' '2015/10/19 8:40' '2015/7/20 15:19'\n",
      " '2015/1/22 19:07' '2014/12/20 14:23' '2014/11/29 18:14'\n",
      " '2014/11/16 17:48' '2014/7/31 17:08' '2014/7/6 16:50' '2014/6/20 11:08'\n",
      " '2013/7/29 10:31']\n",
      "筛选后DataFrame的大小: (0, 10)\n",
      "没有数据满足条件，未能生成Excel文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r\"C:\\Users\\XiLeo\\Desktop\\Soc Media Acqu\\5507 网易云原创榜 GROUP14\\5507 数据文件\\原创榜总comments_data.csv\")\n",
    "\n",
    "# 打印原始DataFrame的大小\n",
    "print(\"原始DataFrame的大小:\", df.shape)\n",
    "\n",
    "# 去除重复的ID\n",
    "df = df.drop_duplicates(subset='user_id')\n",
    "\n",
    "# 打印去重后DataFrame的大小\n",
    "print(\"去重后DataFrame的大小:\", df.shape)\n",
    "\n",
    "# 检查gender列中的唯一值\n",
    "print(\"gender列中的唯一值:\", df['gender'].unique())\n",
    "\n",
    "# 筛选gender列中值为1和2的记录\n",
    "df_filtered = df[df['gender'].isin([1, 2])]\n",
    "\n",
    "# 打印筛选后DataFrame的大小\n",
    "print(\"筛选后DataFrame的大小:\", df_filtered.shape)\n",
    "\n",
    "# 如果DataFrame不为空，则保存清理后的数据到新的Excel文件\n",
    "if not df_filtered.empty:\n",
    "    df_filtered.to_excel('清理后的数据.xlsx', index=False)\n",
    "else:\n",
    "    print(\"没有数据满足条件，未能生成Excel文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0cbb11-772d-46bb-9ba9-83a69e3cdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 定义一个函数来清洗非法字符\n",
    "def clean_text(text):\n",
    "    ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "    return ILLEGAL_CHARACTERS_RE.sub('', str(text))\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r\"C:\\Users\\XiLeo\\Desktop\\Soc Media Acqu\\5507 网易云原创榜 GROUP14\\5507 数据文件\\原创榜总comments_data.csv\")\n",
    "\n",
    "# 使用 DataFrame.map 方法清洗 DataFrame 中的数据\n",
    "df = df.map(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "# 去除重复的ID\n",
    "df = df.drop_duplicates(subset='user_id')\n",
    "\n",
    "# 将gender列转换为数值型，无法转换的设置为NaN\n",
    "df['gender'] = pd.to_numeric(df['gender'], errors='coerce')\n",
    "\n",
    "# 去除转换后为NaN的行，即去除原来不是数值的行\n",
    "df = df.dropna(subset=['gender'])\n",
    "\n",
    "# 筛选gender列中值为1和2的记录\n",
    "df = df[df['gender'].isin([1, 2])]\n",
    "\n",
    "# 保存清理后的数据到新的Excel文件\n",
    "if not df.empty:\n",
    "    df.to_excel('清理后的数据.xlsx', index=False)\n",
    "else:\n",
    "    print(\"没有数据满足条件，未能生成Excel文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9076e39-02f1-4864-8162-34ed85fe2ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 定义一个函数来清洗非法字符并移除无效城市名称\n",
    "def clean_city(text):\n",
    "    # 清洗非法字符\n",
    "    ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "    text = ILLEGAL_CHARACTERS_RE.sub('', str(text))\n",
    "    # 移除空白字符\n",
    "    text = text.strip()\n",
    "    # 检查城市名称是否有效，无效则返回np.nan\n",
    "    if text in [\"未知\", \"无\", \"\"]:\n",
    "        return np.nan\n",
    "    return text\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r\"C:\\Users\\XiLeo\\Desktop\\Soc Media Acqu\\5507 网易云原创榜 GROUP14\\5507 数据文件\\原创榜总comments_data.csv\")\n",
    "\n",
    "# 清洗DataFrame中的数据\n",
    "df['city'] = df['city'].apply(clean_city)\n",
    "\n",
    "# 去除重复的ID\n",
    "df = df.drop_duplicates(subset='user_id')\n",
    "\n",
    "# 去除city列为NaN的行\n",
    "df = df.dropna(subset=['city'])\n",
    "\n",
    "# 保存清理后的数据到新的Excel文件\n",
    "if not df.empty:\n",
    "    df.to_excel('清理后的数据城市2.xlsx', index=False)\n",
    "else:\n",
    "    print(\"没有数据满足条件，未能生成Excel文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ee579f0-64c1-4e2b-91b0-ccd590287a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 定义一个函数来清洗非法字符并移除无效城市名称\n",
    "def clean_city(text):\n",
    "    # 清洗非法字符\n",
    "    ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "    text = ILLEGAL_CHARACTERS_RE.sub('', str(text))\n",
    "    # 移除空白字符\n",
    "    text = text.strip()\n",
    "    # 检查城市名称是否有效，无效则返回np.nan\n",
    "    if text in [\"未知\", \"无\", \"\", \"nan\"]:\n",
    "        return np.nan\n",
    "    return text\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r\"C:\\Users\\XiLeo\\Desktop\\Soc Media Acqu\\5507 网易云原创榜 GROUP14\\5507 数据文件\\原创榜总comments_data.csv\")\n",
    "\n",
    "# 清洗DataFrame中的数据\n",
    "df['city'] = df['city'].apply(clean_city)\n",
    "\n",
    "# 去除重复的ID\n",
    "df = df.drop_duplicates(subset='user_id')\n",
    "\n",
    "\n",
    "# 去除city列为np.nan的行\n",
    "df = df.dropna(subset=['city'])\n",
    "\n",
    "# 去除city列中包含字符串\"nan\"的行\n",
    "df = df[~df['city'].str.contains(\"nan\", case=False)]\n",
    "\n",
    "# 保存清理后的数据到新的Excel文件\n",
    "if not df.empty:\n",
    "    df.to_excel('清理后的数据城市3.xlsx', index=False)\n",
    "else:\n",
    "    print(\"没有数据满足条件，未能生成Excel文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a350fa-501e-4782-9e76-a4eacc7f4aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
