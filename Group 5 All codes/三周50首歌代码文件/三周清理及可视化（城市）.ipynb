{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bb47ac-86ac-4ac7-a525-07d07e08ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyecharts in c:\\users\\xileo\\anaconda3\\lib\\site-packages (2.0.6)\n",
      "Collecting pyecharts\n",
      "  Downloading pyecharts-2.0.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.1.4)\n",
      "Requirement already satisfied: prettytable in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.12.0)\n",
      "Requirement already satisfied: simplejson in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.19.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from jinja2->pyecharts) (2.1.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from prettytable->pyecharts) (0.2.5)\n",
      "Downloading pyecharts-2.0.7-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.0 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/151.0 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 30.7/151.0 kB 325.1 kB/s eta 0:00:01\n",
      "   --------------- ----------------------- 61.4/151.0 kB 469.7 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 122.9/151.0 kB 722.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 151.0/151.0 kB 749.0 kB/s eta 0:00:00\n",
      "Installing collected packages: pyecharts\n",
      "  Attempting uninstall: pyecharts\n",
      "    Found existing installation: pyecharts 2.0.6\n",
      "    Uninstalling pyecharts-2.0.6:\n",
      "      Successfully uninstalled pyecharts-2.0.6\n",
      "Successfully installed pyecharts-2.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyecharts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b301d9-fb1b-43b8-ab39-2c1ee82f0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 假设你的数据框名为df\n",
    "def clean_city_data(df, province_city_map):\n",
    "    # 将邮政编码转换为城市名称\n",
    "    def get_city(code):\n",
    "        try:\n",
    "            code = int(str(code)[:6])  # 取前6位\n",
    "            return province_city_map.get(code, \"未知\")\n",
    "        except:\n",
    "            return \"未知\"\n",
    "    \n",
    "    # 如果有postal_code列，使用它来获取城市名称\n",
    "    if 'postal_code' in df.columns:\n",
    "        df['city'] = df['postal_code'].apply(get_city)\n",
    "    \n",
    "    # 统计每个城市的数量\n",
    "    city_counts = df['city'].value_counts()\n",
    "    \n",
    "    # 绘制柱状图\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    city_counts[:20].plot(kind='bar')\n",
    "    plt.title('各城市数据量分布(Top 20)')\n",
    "    plt.xlabel('城市')\n",
    "    plt.ylabel('数量')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印统计结果\n",
    "    print(\"\\n城市数据统计:\")\n",
    "    print(city_counts)\n",
    "    \n",
    "    return city_counts\n",
    "\n",
    "# 使用示例\n",
    "# df = pd.read_csv('your_data.csv')  # 读取你的数据\n",
    "# city_stats = clean_city_data(df, PROVINCE_CITY_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2b2710-9f87-44eb-9d51-d4068ba52c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XiLeo\\AppData\\Local\\Temp\\ipykernel_8808\\779368980.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if row[1] == '工作簿' and row[2] == '工作表':\n",
      "C:\\Users\\XiLeo\\AppData\\Local\\Temp\\ipykernel_8808\\779368980.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['city'] = data_df['Unnamed: 4'].astype(str).str[:2]  # 取前两位作为编码\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'city_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 4\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 将邮政编码转换为城市名称\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 4\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[:\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# 取前两位作为编码\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(city_map)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# 清理数据，去除任何可能的NaN值\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     data_df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'city_map' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = '数据文件/三周汇总数据.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 查找包含实际数据的起始行\n",
    "start_row = None\n",
    "for i, row in df.iterrows():\n",
    "    if row[1] == '工作簿' and row[2] == '工作表':\n",
    "        start_row = i + 1  # 数据从这一行的下一行开始\n",
    "        break\n",
    "\n",
    "if start_row is not None:\n",
    "    # 从实际数据开始的行切片DataFrame\n",
    "    data_df = df.iloc[start_row:]\n",
    "    \n",
    "    # 假设城市信息在 'Unnamed: 4' 列\n",
    "    if 'Unnamed: 4' in data_df.columns:\n",
    "        # 将邮政编码转换为城市名称\n",
    "        data_df['city'] = data_df['Unnamed: 4'].astype(str).str[:2]  # 取前两位作为编码\n",
    "        data_df['city'] = data_df['city'].map(city_map)\n",
    "        \n",
    "        # 清理数据，去除任何可能的NaN值\n",
    "        data_df.dropna(subset=['city'], inplace=True)\n",
    "        \n",
    "        # 统计每个城市的数据量\n",
    "        city_counts = data_df['city'].value_counts()\n",
    "        \n",
    "        # 绘制条形图\n",
    "        city_counts.plot(kind='bar', figsize=(10, 8))\n",
    "        plt.title('City Data Counts')\n",
    "        plt.xlabel('City')\n",
    "        plt.ylabel('Counts')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()  # 调整布局以适应标签\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"DataFrame中没有找到包含城市信息的列，请检查列名是否正确\")\n",
    "else:\n",
    "    print(\"未找到包含实际数据的行\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6fec04-f4cd-4014-b8ce-a3eb07720573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理文件时出错: [Errno 2] No such file or directory: 'input.xlsx'\n",
      "未找到有效数据或处理过程出错\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 定义城市编码映射\n",
    "city_map = {\n",
    "    '10': '北京', '11': '北京', '12': '天津', '13': '河北', '14': '山西', '15': '内蒙古',\n",
    "    '21': '辽宁', '22': '吉林', '23': '黑龙江', \n",
    "    '31': '上海', '32': '江苏', '33': '浙江', '34': '安徽', '35': '福建', '36': '江西', '37': '山东',\n",
    "    '41': '河南', '42': '湖北', '43': '湖南', '44': '广东', '45': '广西', '46': '海南',\n",
    "    '50': '重庆', '51': '四川', '52': '贵州', '53': '云南', '54': '西藏',\n",
    "    '61': '陕西', '62': '甘肃', '63': '青海', '64': '宁夏', '65': '新疆',\n",
    "    '71': '台湾', '81': '香港', '82': '澳门'\n",
    "}\n",
    "\n",
    "def process_excel(file_path):\n",
    "    try:\n",
    "        # 读取所有工作表\n",
    "        all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "        \n",
    "        # 用于存储所有符合条件的数据\n",
    "        all_data = []\n",
    "        \n",
    "        # 遍历所有工作表\n",
    "        for sheet_name, sheet_data in all_sheets.items():\n",
    "            # 检查工作表是否包含所需列\n",
    "            if 'Unnamed: 4' in sheet_data.columns:\n",
    "                # 创建数据副本以避免SettingWithCopyWarning\n",
    "                df = sheet_data.copy()\n",
    "                \n",
    "                # 提取邮政编码前两位\n",
    "                df['city_code'] = df['Unnamed: 4'].astype(str).str[:2]\n",
    "                \n",
    "                # 使用映射转换为城市名称\n",
    "                df['city'] = df['city_code'].map(city_map)\n",
    "                \n",
    "                # 删除临时的city_code列\n",
    "                df = df.drop('city_code', axis=1)\n",
    "                \n",
    "                # 清理数据，去除NaN值\n",
    "                df = df.dropna(subset=['city'])\n",
    "                \n",
    "                all_data.append(df)\n",
    "        \n",
    "        # 合并所有数据\n",
    "        if all_data:\n",
    "            final_df = pd.concat(all_data, ignore_index=True)\n",
    "            \n",
    "            # 按城市分组并计算统计信息\n",
    "            city_stats = final_df.groupby('city').agg({\n",
    "                'Unnamed: 1': 'count',  # 假设这是需要计数的列\n",
    "                'Unnamed: 2': 'sum'     # 假设这是需要求和的列\n",
    "            }).reset_index()\n",
    "            \n",
    "            # 重命名列\n",
    "            city_stats.columns = ['城市', '数量', '总和']\n",
    "            \n",
    "            return city_stats\n",
    "            \n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时出错: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def export_results(city_stats, output_path):\n",
    "    try:\n",
    "        # 导出到Excel\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            city_stats.to_excel(writer, sheet_name='城市统计', index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"导出结果时出错: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    input_file = \"input.xlsx\"  # 输入文件路径\n",
    "    output_file = \"output.xlsx\"  # 输出文件路径\n",
    "    \n",
    "    # 处理数据\n",
    "    results = process_excel(input_file)\n",
    "    \n",
    "    if not results.empty:\n",
    "        # 导出结果\n",
    "        if export_results(results, output_file):\n",
    "            print(\"处理完成，结果已导出到\", output_file)\n",
    "        else:\n",
    "            print(\"导出结果失败\")\n",
    "    else:\n",
    "        print(\"未找到有效数据或处理过程出错\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9620e272-1e0d-4508-b9bb-93c781c8f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始执行程序...\n",
      "正在处理文件: 数据文件\\三周汇总数据.xlsx\n",
      "成功读取到 2 个工作表\n",
      "正在处理工作表: 报告\n",
      "工作表列名: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "工作表 报告 不包含邮编列\n",
      "正在处理工作表: 总表\n",
      "工作表列名: ['Unnamed: 0', 'user_id', 'age', 'gender', 'city', 'Unnamed: 5']\n",
      "工作表 总表 不包含邮编列\n",
      "未找到任何有效数据\n",
      "未找到有效数据或处理过程出错\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义城市编码映射\n",
    "city_map = {\n",
    "    '10': '北京', '11': '北京', '12': '天津', '13': '河北', '14': '山西', '15': '内蒙古',\n",
    "    '21': '辽宁', '22': '吉林', '23': '黑龙江', \n",
    "    '31': '上海', '32': '江苏', '33': '浙江', '34': '安徽', '35': '福建', '36': '江西', '37': '山东',\n",
    "    '41': '河南', '42': '湖北', '43': '湖南', '44': '广东', '45': '广西', '46': '海南',\n",
    "    '50': '重庆', '51': '四川', '52': '贵州', '53': '云南', '54': '西藏',\n",
    "    '61': '陕西', '62': '甘肃', '63': '青海', '64': '宁夏', '65': '新疆',\n",
    "    '71': '台湾', '81': '香港', '82': '澳门'\n",
    "}\n",
    "\n",
    "def process_excel(file_path):\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"文件不存在: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"正在处理文件: {file_path}\")\n",
    "        \n",
    "        # 读取所有工作表\n",
    "        all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "        print(f\"成功读取到 {len(all_sheets)} 个工作表\")\n",
    "        \n",
    "        # 用于存储所有符合条件的数据\n",
    "        all_data = []\n",
    "        \n",
    "        # 遍历所有工作表\n",
    "        for sheet_name, sheet_data in all_sheets.items():\n",
    "            print(f\"正在处理工作表: {sheet_name}\")\n",
    "            \n",
    "            # 打印列名，用于调试\n",
    "            print(f\"工作表列名: {sheet_data.columns.tolist()}\")\n",
    "            \n",
    "            # 检查工作表是否包含所需列\n",
    "            postal_code_column = None\n",
    "            for col in sheet_data.columns:\n",
    "                if isinstance(col, str) and '邮编' in col:\n",
    "                    postal_code_column = col\n",
    "                    break\n",
    "            \n",
    "            if postal_code_column:\n",
    "                # 创建数据副本以避免SettingWithCopyWarning\n",
    "                df = sheet_data.copy()\n",
    "                \n",
    "                # 提取邮政编码前两位\n",
    "                df['city_code'] = df[postal_code_column].astype(str).str[:2]\n",
    "                \n",
    "                # 使用映射转换为城市名称\n",
    "                df['city'] = df['city_code'].map(city_map)\n",
    "                \n",
    "                # 删除临时的city_code列\n",
    "                df = df.drop('city_code', axis=1)\n",
    "                \n",
    "                # 清理数据，去除NaN值\n",
    "                df = df.dropna(subset=['city'])\n",
    "                \n",
    "                print(f\"工作表 {sheet_name} 处理完成，找到 {len(df)} 条有效数据\")\n",
    "                all_data.append(df)\n",
    "            else:\n",
    "                print(f\"工作表 {sheet_name} 不包含邮编列\")\n",
    "        \n",
    "        # 合并所有数据\n",
    "        if all_data:\n",
    "            final_df = pd.concat(all_data, ignore_index=True)\n",
    "            print(f\"总共处理了 {len(final_df)} 条数据\")\n",
    "            \n",
    "            # 按城市分组并计算统计信息\n",
    "            # 获取数值类型的列用于统计\n",
    "            numeric_columns = final_df.select_dtypes(include=[np.number]).columns\n",
    "            if len(numeric_columns) > 0:\n",
    "                first_numeric_col = numeric_columns[0]\n",
    "                city_stats = final_df.groupby('city').agg({\n",
    "                    first_numeric_col: ['count', 'sum']\n",
    "                }).reset_index()\n",
    "                \n",
    "                # 重命名列\n",
    "                city_stats.columns = ['城市', '数量', '总和']\n",
    "            else:\n",
    "                # 如果没有数值列，只计算数量\n",
    "                city_stats = final_df.groupby('city').size().reset_index(name='数量')\n",
    "                city_stats['总和'] = 0\n",
    "            \n",
    "            print(\"数据处理完成\")\n",
    "            return city_stats\n",
    "            \n",
    "        else:\n",
    "            print(\"未找到任何有效数据\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时出错: {str(e)}\")\n",
    "        print(\"错误详细信息:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def export_results(city_stats, output_path):\n",
    "    try:\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # 导出到Excel\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            city_stats.to_excel(writer, sheet_name='城市统计', index=False)\n",
    "        print(f\"结果已成功导出到: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"导出结果时出错: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    print(\"开始执行程序...\")\n",
    "    \n",
    "    # 使用指定的文件路径\n",
    "    input_file = os.path.join(\"数据文件\", \"三周汇总数据.xlsx\")\n",
    "    output_file = os.path.join(\"数据文件\", \"城市统计结果.xlsx\")\n",
    "    \n",
    "    # 处理数据\n",
    "    results = process_excel(input_file)\n",
    "    \n",
    "    if not results.empty:\n",
    "        # 导出结果\n",
    "        if export_results(results, output_file):\n",
    "            print(\"处理完成，结果已导出\")\n",
    "            # 显示处理结果预览\n",
    "            print(\"\\n处理结果预览:\")\n",
    "            print(results)\n",
    "        else:\n",
    "            print(\"导出结果失败\")\n",
    "    else:\n",
    "        print(\"未找到有效数据或处理过程出错\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9d53ac-a43c-42ab-9f1d-23c4c817943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始执行程序...\n",
      "正在处理文件: 数据文件\\三周汇总数据.xlsx\n",
      "成功读取到 2 个工作表\n",
      "正在处理工作表: 报告\n",
      "工作表列名: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "\n",
      "前几行数据预览:\n",
      "   Unnamed: 0                      Unnamed: 1      Unnamed: 2 Unnamed: 3  \\\n",
      "0         NaN  报告：成功合并 102 个工作表，共 300838 行数据。             NaN        NaN   \n",
      "1         NaN                             NaN             NaN        NaN   \n",
      "2         NaN                             工作簿             工作表       合并状态   \n",
      "3         NaN              comments_data1.csv  comments_data1         成功   \n",
      "4         NaN              comments_data2.csv  comments_data2         成功   \n",
      "\n",
      "         Unnamed: 4  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2            合并后的位置  \n",
      "3      总表!B1:F10200  \n",
      "4  总表!B10201:F15300  \n",
      "正在处理工作表: 总表\n",
      "工作表列名: ['Unnamed: 0', 'user_id', 'age', 'gender', 'city', 'Unnamed: 5']\n",
      "\n",
      "前几行数据预览:\n",
      "                       Unnamed: 0     user_id age gender    city Unnamed: 5\n",
      "0  [comments_data1]comments_data1   544930017   0      1     100        NaN\n",
      "1  [comments_data1]comments_data1   544930017   0      1     100        NaN\n",
      "2  [comments_data1]comments_data1  8500945847   0      0  441200        NaN\n",
      "3  [comments_data1]comments_data1  8500945847   0      0  441200        NaN\n",
      "4  [comments_data1]comments_data1  8500945847   0      0  441200        NaN\n",
      "总表处理完成，找到 300838 条数据\n",
      "总共处理了 300838 条数据\n",
      "处理文件时出错: agg function failed [how->mean,dtype->object]\n",
      "错误详细信息:\n",
      "未找到有效数据或处理过程出错\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 1942, in _agg_py_fallback\n",
      "    res_values = self._grouper.agg_series(ser, alt, preserve_dtype=True)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py\", line 864, in agg_series\n",
      "    result = self._aggregate_series_pure_python(obj, func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py\", line 885, in _aggregate_series_pure_python\n",
      "    res = func(group)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 2454, in <lambda>\n",
      "    alt=lambda x: Series(x, copy=False).mean(numeric_only=numeric_only),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py\", line 6549, in mean\n",
      "    return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 12420, in mean\n",
      "    return self._stat_function(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 12377, in _stat_function\n",
      "    return self._reduce(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py\", line 6457, in _reduce\n",
      "    return op(delegate, skipna=skipna, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 147, in f\n",
      "    result = alt(values, axis=axis, skipna=skipna, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 404, in new_func\n",
      "    result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 719, in nanmean\n",
      "    the_sum = values.sum(axis, dtype=dtype_sum)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py\", line 49, in _sum\n",
      "    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: can only concatenate str (not \"int\") to str\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\XiLeo\\AppData\\Local\\Temp\\ipykernel_8808\\61449968.py\", line 64, in process_excel\n",
      "    city_stats = final_df.groupby('city').agg({\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\", line 1432, in aggregate\n",
      "    result = op.agg()\n",
      "             ^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py\", line 190, in agg\n",
      "    return self.agg_dict_like()\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py\", line 423, in agg_dict_like\n",
      "    return self.agg_or_apply_dict_like(op_name=\"agg\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1608, in agg_or_apply_dict_like\n",
      "    result_index, result_data = self.compute_dict_like(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py\", line 497, in compute_dict_like\n",
      "    getattr(obj._gotitem(key, ndim=1), op_name)(how, **kwargs)\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\", line 257, in aggregate\n",
      "    ret = self._aggregate_multiple_funcs(func, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\", line 362, in _aggregate_multiple_funcs\n",
      "    results[key] = self.aggregate(func, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\", line 249, in aggregate\n",
      "    return getattr(self, func)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 2452, in mean\n",
      "    result = self._cython_agg_general(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 1998, in _cython_agg_general\n",
      "    new_mgr = data.grouped_reduce(array_func)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\base.py\", line 367, in grouped_reduce\n",
      "    res = func(arr)\n",
      "          ^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 1995, in array_func\n",
      "    result = self._agg_py_fallback(how, values, ndim=data.ndim, alt=alt)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\XiLeo\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 1946, in _agg_py_fallback\n",
      "    raise type(err)(msg) from err\n",
      "TypeError: agg function failed [how->mean,dtype->object]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义城市编码映射\n",
    "city_map = {\n",
    "    '10': '北京', '11': '北京', '12': '天津', '13': '河北', '14': '山西', '15': '内蒙古',\n",
    "    '21': '辽宁', '22': '吉林', '23': '黑龙江', \n",
    "    '31': '上海', '32': '江苏', '33': '浙江', '34': '安徽', '35': '福建', '36': '江西', '37': '山东',\n",
    "    '41': '河南', '42': '湖北', '43': '湖南', '44': '广东', '45': '广西', '46': '海南',\n",
    "    '50': '重庆', '51': '四川', '52': '贵州', '53': '云南', '54': '西藏',\n",
    "    '61': '陕西', '62': '甘肃', '63': '青海', '64': '宁夏', '65': '新疆',\n",
    "    '71': '台湾', '81': '香港', '82': '澳门'\n",
    "}\n",
    "\n",
    "def process_excel(file_path):\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"文件不存在: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"正在处理文件: {file_path}\")\n",
    "        \n",
    "        # 读取所有工作表\n",
    "        all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "        print(f\"成功读取到 {len(all_sheets)} 个工作表\")\n",
    "        \n",
    "        # 用于存储所有符合条件的数据\n",
    "        all_data = []\n",
    "        \n",
    "        # 遍历所有工作表\n",
    "        for sheet_name, sheet_data in all_sheets.items():\n",
    "            print(f\"正在处理工作表: {sheet_name}\")\n",
    "            \n",
    "            # 打印列名和前几行数据，用于调试\n",
    "            print(f\"工作表列名: {sheet_data.columns.tolist()}\")\n",
    "            print(\"\\n前几行数据预览:\")\n",
    "            print(sheet_data.head())\n",
    "            \n",
    "            if 'Unnamed: 4' in sheet_data.columns:  # 处理报告工作表\n",
    "                df = sheet_data.copy()\n",
    "                postal_codes = df['Unnamed: 4'].astype(str)\n",
    "                valid_codes = postal_codes.str.match(r'^\\d{6}$')  # 检查是否为6位数字\n",
    "                df = df[valid_codes]\n",
    "                if not df.empty:\n",
    "                    df['city_code'] = df['Unnamed: 4'].astype(str).str[:2]\n",
    "                    df['city'] = df['city_code'].map(city_map)\n",
    "                    df = df.drop('city_code', axis=1)\n",
    "                    all_data.append(df)\n",
    "                    print(f\"报告表处理完成，找到 {len(df)} 条有效数据\")\n",
    "            \n",
    "            elif 'city' in sheet_data.columns:  # 处理总表工作表\n",
    "                df = sheet_data.copy()\n",
    "                all_data.append(df)\n",
    "                print(f\"总表处理完成，找到 {len(df)} 条数据\")\n",
    "        \n",
    "        # 合并所有数据\n",
    "        if all_data:\n",
    "            final_df = pd.concat(all_data, ignore_index=True)\n",
    "            print(f\"总共处理了 {len(final_df)} 条数据\")\n",
    "            \n",
    "            # 按城市分组并计算统计信息\n",
    "            if 'age' in final_df.columns:  # 如果有age列，计算年龄统计\n",
    "                city_stats = final_df.groupby('city').agg({\n",
    "                    'age': ['count', 'mean']\n",
    "                }).reset_index()\n",
    "                city_stats.columns = ['城市', '数量', '平均年龄']\n",
    "            else:  # 否则只计算数量\n",
    "                city_stats = final_df.groupby('city').size().reset_index(name='数量')\n",
    "            \n",
    "            print(\"数据处理完成\")\n",
    "            return city_stats\n",
    "            \n",
    "        else:\n",
    "            print(\"未找到任何有效数据\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时出错: {str(e)}\")\n",
    "        print(\"错误详细信息:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def export_results(city_stats, output_path):\n",
    "    try:\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # 导出到Excel\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            city_stats.to_excel(writer, sheet_name='城市统计', index=False)\n",
    "            \n",
    "            # 添加数据分析\n",
    "            if 'average_age' in city_stats.columns:\n",
    "                analysis = pd.DataFrame({\n",
    "                    '统计指标': ['总人数', '城市数量', '平均年龄'],\n",
    "                    '值': [\n",
    "                        city_stats['数量'].sum(),\n",
    "                        len(city_stats),\n",
    "                        city_stats['平均年龄'].mean()\n",
    "                    ]\n",
    "                })\n",
    "                analysis.to_excel(writer, sheet_name='数据分析', index=False)\n",
    "            \n",
    "        print(f\"结果已成功导出到: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"导出结果时出错: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    print(\"开始执行程序...\")\n",
    "    \n",
    "    # 使用指定的文件路径\n",
    "    input_file = os.path.join(\"数据文件\", \"三周汇总数据.xlsx\")\n",
    "    output_file = os.path.join(\"数据文件\", \"城市统计结果.xlsx\")\n",
    "    \n",
    "    # 处理数据\n",
    "    results = process_excel(input_file)\n",
    "    \n",
    "    if not results.empty:\n",
    "        # 导出结果\n",
    "        if export_results(results, output_file):\n",
    "            print(\"处理完成，结果已导出\")\n",
    "            # 显示处理结果预览\n",
    "            print(\"\\n处理结果预览:\")\n",
    "            print(results)\n",
    "        else:\n",
    "            print(\"导出结果失败\")\n",
    "    else:\n",
    "        print(\"未找到有效数据或处理过程出错\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0636e1c5-031b-4d7d-9453-855798356f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始执行程序...\n",
      "正在处理文件: 数据文件\\三周汇总数据.xlsx\n",
      "成功读取到 2 个工作表\n",
      "正在处理工作表: 报告\n",
      "工作表列名: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "\n",
      "前几行数据预览:\n",
      "   Unnamed: 0                      Unnamed: 1      Unnamed: 2 Unnamed: 3  \\\n",
      "0         NaN  报告：成功合并 102 个工作表，共 300838 行数据。             NaN        NaN   \n",
      "1         NaN                             NaN             NaN        NaN   \n",
      "2         NaN                             工作簿             工作表       合并状态   \n",
      "3         NaN              comments_data1.csv  comments_data1         成功   \n",
      "4         NaN              comments_data2.csv  comments_data2         成功   \n",
      "\n",
      "         Unnamed: 4  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2            合并后的位置  \n",
      "3      总表!B1:F10200  \n",
      "4  总表!B10201:F15300  \n",
      "正在处理工作表: 总表\n",
      "工作表列名: ['Unnamed: 0', 'user_id', 'age', 'gender', 'city', 'Unnamed: 5']\n",
      "\n",
      "前几行数据预览:\n",
      "                       Unnamed: 0     user_id age gender    city Unnamed: 5\n",
      "0  [comments_data1]comments_data1   544930017   0      1     100        NaN\n",
      "1  [comments_data1]comments_data1   544930017   0      1     100        NaN\n",
      "2  [comments_data1]comments_data1  8500945847   0      0  441200        NaN\n",
      "3  [comments_data1]comments_data1  8500945847   0      0  441200        NaN\n",
      "4  [comments_data1]comments_data1  8500945847   0      0  441200        NaN\n",
      "总表处理完成，找到 50837 条数据\n",
      "总共处理了 50837 条数据\n",
      "数据处理完成\n",
      "结果已成功导出到: 数据文件\\城市统计结果.xlsx\n",
      "处理完成，结果已导出\n",
      "\n",
      "处理结果预览:\n",
      "     城市    数量   平均年龄   性别计数\n",
      "0    上海  1023  18.08   1377\n",
      "1    云南   728  21.54   1071\n",
      "2   内蒙古   257  24.09    417\n",
      "3    北京  5728  17.63  17574\n",
      "4    台湾   157  20.68    182\n",
      "5    吉林   516  11.68    654\n",
      "6    四川  3658  25.95   4379\n",
      "7    天津   357  14.53    544\n",
      "8    宁夏   136  20.91    159\n",
      "9    安徽   621  17.92    879\n",
      "10   山东  1083  18.93   1808\n",
      "11   山西   261  19.99    379\n",
      "12   广东  2507  19.08   4406\n",
      "13   广西   399  22.53    659\n",
      "14   新疆   186  20.39    267\n",
      "15   江苏  1505  19.60   2279\n",
      "16   江西   559  20.20    785\n",
      "17   河北   571  19.07    920\n",
      "18   河南  1038  18.61   1597\n",
      "19   浙江  1652  18.68   2521\n",
      "20   海南    92  21.92    149\n",
      "21   湖北   725  20.55   1121\n",
      "22   湖南   499  20.77   1278\n",
      "23   澳门    25  17.40     31\n",
      "24   甘肃   176  23.63    239\n",
      "25   福建   858  12.51   1376\n",
      "26   西藏   117  25.82    210\n",
      "27   贵州   273  23.78    396\n",
      "28   辽宁   452  16.98    706\n",
      "29   重庆   605  19.48    827\n",
      "30   陕西   389  20.21    588\n",
      "31   青海   148  21.23    168\n",
      "32   香港   111  21.80    287\n",
      "33  黑龙江   458  20.49    604\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义城市编码映射\n",
    "city_map = {\n",
    "    '10': '北京', '11': '北京', '12': '天津', '13': '河北', '14': '山西', '15': '内蒙古',\n",
    "    '21': '辽宁', '22': '吉林', '23': '黑龙江', \n",
    "    '31': '上海', '32': '江苏', '33': '浙江', '34': '安徽', '35': '福建', '36': '江西', '37': '山东',\n",
    "    '41': '河南', '42': '湖北', '43': '湖南', '44': '广东', '45': '广西', '46': '海南',\n",
    "    '50': '重庆', '51': '四川', '52': '贵州', '53': '云南', '54': '西藏',\n",
    "    '61': '陕西', '62': '甘肃', '63': '青海', '64': '宁夏', '65': '新疆',\n",
    "    '71': '台湾', '81': '香港', '82': '澳门'\n",
    "}\n",
    "\n",
    "def clean_city_code(code):\n",
    "    # 转换为字符串并取前两位\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    code_str = str(code)\n",
    "    if len(code_str) >= 2:\n",
    "        return code_str[:2]\n",
    "    return None\n",
    "\n",
    "def clean_age(age):\n",
    "    try:\n",
    "        age_val = float(age)\n",
    "        if 0 <= age_val <= 120:  # 合理的年龄范围\n",
    "            return age_val\n",
    "        return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def process_excel(file_path):\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"文件不存在: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"正在处理文件: {file_path}\")\n",
    "        \n",
    "        # 读取所有工作表\n",
    "        all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "        print(f\"成功读取到 {len(all_sheets)} 个工作表\")\n",
    "        \n",
    "        # 用于存储所有符合条件的数据\n",
    "        all_data = []\n",
    "        \n",
    "        # 遍历所有工作表\n",
    "        for sheet_name, sheet_data in all_sheets.items():\n",
    "            print(f\"正在处理工作表: {sheet_name}\")\n",
    "            print(f\"工作表列名: {sheet_data.columns.tolist()}\")\n",
    "            print(\"\\n前几行数据预览:\")\n",
    "            print(sheet_data.head())\n",
    "            \n",
    "            if 'city' in sheet_data.columns:  # 处理总表工作表\n",
    "                df = sheet_data.copy()\n",
    "                \n",
    "                # 清理和转换城市代码\n",
    "                df['city_code'] = df['city'].apply(clean_city_code)\n",
    "                df['city_name'] = df['city_code'].map(city_map)\n",
    "                \n",
    "                # 清理年龄数据\n",
    "                df['age_clean'] = df['age'].apply(clean_age)\n",
    "                \n",
    "                # 只保留有效的数据行\n",
    "                df = df.dropna(subset=['city_name'])\n",
    "                \n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "                    print(f\"总表处理完成，找到 {len(df)} 条数据\")\n",
    "        \n",
    "        # 合并所有数据\n",
    "        if all_data:\n",
    "            final_df = pd.concat(all_data, ignore_index=True)\n",
    "            print(f\"总共处理了 {len(final_df)} 条数据\")\n",
    "            \n",
    "            # 按城市分组统计\n",
    "            city_stats = (final_df.groupby('city_name')\n",
    "                         .agg({\n",
    "                             'age_clean': ['count', lambda x: x.mean(skipna=True)],\n",
    "                             'gender': 'count'\n",
    "                         })\n",
    "                         .reset_index())\n",
    "            \n",
    "            # 重命名列\n",
    "            city_stats.columns = ['城市', '数量', '平均年龄', '性别计数']\n",
    "            \n",
    "            # 处理可能的NaN值\n",
    "            city_stats['平均年龄'] = city_stats['平均年龄'].round(2)\n",
    "            city_stats = city_stats.fillna({'平均年龄': 0})\n",
    "            \n",
    "            print(\"数据处理完成\")\n",
    "            return city_stats\n",
    "            \n",
    "        else:\n",
    "            print(\"未找到任何有效数据\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时出错: {str(e)}\")\n",
    "        print(\"错误详细信息:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def export_results(city_stats, output_path):\n",
    "    try:\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # 导出到Excel\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            city_stats.to_excel(writer, sheet_name='城市统计', index=False)\n",
    "            \n",
    "            # 添加数据分析\n",
    "            analysis = pd.DataFrame({\n",
    "                '统计指标': ['总人数', '城市数量', '总体平均年龄'],\n",
    "                '值': [\n",
    "                    city_stats['数量'].sum(),\n",
    "                    len(city_stats),\n",
    "                    (city_stats['平均年龄'] * city_stats['数量']).sum() / city_stats['数量'].sum()\n",
    "                ]\n",
    "            })\n",
    "            analysis.to_excel(writer, sheet_name='数据分析', index=False)\n",
    "            \n",
    "        print(f\"结果已成功导出到: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"导出结果时出错: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    print(\"开始执行程序...\")\n",
    "    \n",
    "    # 使用指定的文件路径\n",
    "    input_file = os.path.join(\"数据文件\", \"三周汇总数据.xlsx\")\n",
    "    output_file = os.path.join(\"数据文件\", \"城市统计结果.xlsx\")\n",
    "    \n",
    "    # 处理数据\n",
    "    results = process_excel(input_file)\n",
    "    \n",
    "    if not results.empty:\n",
    "        # 导出结果\n",
    "        if export_results(results, output_file):\n",
    "            print(\"处理完成，结果已导出\")\n",
    "            # 显示处理结果预览\n",
    "            print(\"\\n处理结果预览:\")\n",
    "            print(results)\n",
    "        else:\n",
    "            print(\"导出结果失败\")\n",
    "    else:\n",
    "        print(\"未找到有效数据或处理过程出错\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b484d1b-79c7-420b-a7cb-9659398812bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\xileo\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\xileo\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-1.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: plotly in c:\\users\\xileo\\anaconda3\\lib\\site-packages (5.22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas)\n",
      "  Downloading pyogrio-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting pyproj>=3.3.0 (from geopandas)\n",
      "  Downloading pyproj-3.7.0-cp312-cp312-win_amd64.whl.metadata (31 kB)\n",
      "Collecting shapely>=2.0.0 (from geopandas)\n",
      "  Downloading shapely-2.0.6-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyogrio>=0.7.2->geopandas) (2024.6.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading geopandas-1.0.1-py3-none-any.whl (323 kB)\n",
      "   ---------------------------------------- 0.0/323.6 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 41.0/323.6 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 81.9/323.6 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 163.8/323.6 kB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 225.3/323.6 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 286.7/323.6 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 323.6/323.6 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading pyogrio-0.10.0-cp312-cp312-win_amd64.whl (16.2 MB)\n",
      "   ---------------------------------------- 0.0/16.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/16.2 MB 3.4 MB/s eta 0:00:05\n",
      "   ---------------------------------------- 0.1/16.2 MB 2.6 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 0.2/16.2 MB 1.6 MB/s eta 0:00:11\n",
      "    --------------------------------------- 0.2/16.2 MB 1.4 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.3/16.2 MB 1.6 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.4/16.2 MB 1.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/16.2 MB 1.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/16.2 MB 1.6 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.6/16.2 MB 1.6 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.7/16.2 MB 1.6 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.8/16.2 MB 1.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.8/16.2 MB 1.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.9/16.2 MB 1.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.0/16.2 MB 1.7 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.1/16.2 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.2/16.2 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.3/16.2 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.4/16.2 MB 1.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.5/16.2 MB 1.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.6/16.2 MB 1.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.7/16.2 MB 1.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.8/16.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.0/16.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 2.1/16.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 2.2/16.2 MB 2.0 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 2.3/16.2 MB 2.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.4/16.2 MB 2.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.5/16.2 MB 2.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.7/16.2 MB 2.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.8/16.2 MB 2.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 3.0/16.2 MB 2.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 3.1/16.2 MB 2.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.3/16.2 MB 2.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 3.4/16.2 MB 2.3 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 3.6/16.2 MB 2.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.8/16.2 MB 2.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.9/16.2 MB 2.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 4.1/16.2 MB 2.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 4.2/16.2 MB 2.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.5/16.2 MB 2.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.6/16.2 MB 2.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.8/16.2 MB 2.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.0/16.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 5.3/16.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 5.5/16.2 MB 2.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.7/16.2 MB 2.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.9/16.2 MB 2.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 6.1/16.2 MB 2.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 6.3/16.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.5/16.2 MB 2.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.8/16.2 MB 2.9 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 7.1/16.2 MB 3.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.4/16.2 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.6/16.2 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.9/16.2 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.1/16.2 MB 3.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.4/16.2 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.7/16.2 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.9/16.2 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.3/16.2 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.5/16.2 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.9/16.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.1/16.2 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.5/16.2 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.8/16.2 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.2/16.2 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.5/16.2 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.7/16.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.2/16.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.5/16.2 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.7/16.2 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.3/16.2 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.6/16.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/16.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.4/16.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.9/16.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.3/16.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/16.2 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.2/16.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.2/16.2 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading pyproj-3.7.0-cp312-cp312-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/6.2 MB 10.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.9/6.2 MB 14.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.1/6.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.9/6.2 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.4/6.2 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.0/6.2 MB 11.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.6/6.2 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.0/6.2 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.8/6.2 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.2 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.2 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading shapely-2.0.6-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.7/1.4 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 15.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 13.0 MB/s eta 0:00:00\n",
      "Installing collected packages: shapely, pyproj, pyogrio, geopandas\n",
      "Successfully installed geopandas-1.0.1 pyogrio-0.10.0 pyproj-3.7.0 shapely-2.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib pandas geopandas plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c4489a-3cdb-4d03-9b8f-a1eaec41f20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\xileo\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pyecharts in c:\\users\\xileo\\anaconda3\\lib\\site-packages (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.1.4)\n",
      "Requirement already satisfied: prettytable in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.12.0)\n",
      "Requirement already satisfied: simplejson in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.19.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from jinja2->pyecharts) (2.1.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from prettytable->pyecharts) (0.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas pyecharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16749fbb-83d8-4189-8c90-3dc2c3f501e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts.charts import Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66d24ddb-4dd4-4bd9-ba39-d0ce7b054219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyecharts in c:\\users\\xileo\\anaconda3\\lib\\site-packages (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.1.4)\n",
      "Requirement already satisfied: prettytable in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.12.0)\n",
      "Requirement already satisfied: simplejson in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from pyecharts) (3.19.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from jinja2->pyecharts) (2.1.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\xileo\\anaconda3\\lib\\site-packages (from prettytable->pyecharts) (0.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pyecharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f6bbfbb-c960-4ce0-8365-83f3ea8c4b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\XiLeo\\\\城市人数分布图.html'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyecharts.charts import Map\n",
    "from pyecharts import options as opts\n",
    "\n",
    "# 映射函数，将你的城市名称映射到pyecharts地图中的全称\n",
    "def map_city_name(city_name):\n",
    "    city_mapping = {\n",
    "        \"上海\": \"上海市\",\n",
    "        \"云南\": \"云南省\",\n",
    "        \"内蒙古\": \"内蒙古自治区\",\n",
    "        \"北京\": \"北京市\",\n",
    "        \"台湾\": \"台湾省\",\n",
    "        \"吉林\": \"吉林省\",\n",
    "        \"四川\": \"四川省\",\n",
    "        \"天津\": \"天津市\",\n",
    "        \"宁夏\": \"宁夏回族自治区\",\n",
    "        \"安徽\": \"安徽省\",\n",
    "        \"山东\": \"山东省\",\n",
    "        \"山西\": \"山西省\",\n",
    "        \"广东\": \"广东省\",\n",
    "        \"广西\": \"广西壮族自治区\",\n",
    "        \"新疆\": \"新疆维吾尔自治区\",\n",
    "        \"江苏\": \"江苏省\",\n",
    "        \"江西\": \"江西省\",\n",
    "        \"河北\": \"河北省\",\n",
    "        \"河南\": \"河南省\",\n",
    "        \"浙江\": \"浙江省\",\n",
    "        \"海南\": \"海南省\",\n",
    "        \"湖北\": \"湖北省\",\n",
    "        \"湖南\": \"湖南省\",\n",
    "        \"澳门\": \"澳门特别行政区\",\n",
    "        \"甘肃\": \"甘肃省\",\n",
    "        \"福建\": \"福建省\",\n",
    "        \"西藏\": \"西藏自治区\",\n",
    "        \"贵州\": \"贵州省\",\n",
    "        \"辽宁\": \"辽宁省\",\n",
    "        \"重庆\": \"重庆市\",\n",
    "        \"陕西\": \"陕西省\",\n",
    "        \"青海\": \"青海省\",\n",
    "        \"香港\": \"香港特别行政区\",\n",
    "        \"黑龙江\": \"黑龙江省\"\n",
    "    }\n",
    "    return city_mapping.get(city_name, city_name)\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(\"数据文件\\\\城市统计结果.xlsx\")\n",
    "\n",
    "# 应用映射函数\n",
    "df['城市'] = df['城市'].apply(map_city_name)\n",
    "\n",
    "# 准备地图数据\n",
    "attr = df['城市'].tolist()\n",
    "value = df['数量'].tolist()\n",
    "\n",
    "# 绘制地图\n",
    "map = Map()\n",
    "map.add(\"城市人数\", [list(z) for z in zip(attr, value)], \"china\", is_map_symbol_show=True)\n",
    "map.set_global_opts(\n",
    "    title_opts=opts.TitleOpts(title=\"城市人数分布图\"),\n",
    "    visualmap_opts=opts.VisualMapOpts(is_show=True, is_piecewise=True, min_=min(value), max_=max(value))\n",
    ")\n",
    "map.render('城市人数分布图.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f75c5-feb9-4c15-a455-537ab96294ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
